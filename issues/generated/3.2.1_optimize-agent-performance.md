# 3.2.1 – Optimize agent performance

## Summary (Work Breakdown)

- **Description:** Profile and improve end-to-end latency for the multi-source agent, introducing caching and concurrency safeguards.
- **Work Package:** 3.2 Performance Optimization
- **Duration:** 3 hours
- **Owner(s):** TBD

- **Context:** The agent must deliver insights in under two minutes (see Project Charter). Current prototype executes sources sequentially and lacks observability. This story instruments the stack, targets the highest-cost segments, and applies low-risk optimisations before we scale to additional providers.

## Acceptance Criteria
- [ ] Add lightweight timing/trace instrumentation (e.g., `structlog`, OpenTelemetry stubs, or custom timers) around tool execution, LLM calls, and post-processing; metrics must surface in logs and an optional JSON diagnostics block.
- [ ] Introduce parallel execution for independent tools (reddit/twitter/google) with bounded worker pools to avoid API rate spikes; configuration toggles must live in `config/settings.py`.
- [ ] Implement response caching for idempotent calls (e.g., Reddit listings) using an in-memory cache interface with pluggable backends (future Redis), respecting cache TTL configuration.
- [ ] Document tuning levers (worker count, cache TTL, timeout budgets) in `docs/ops/performance-playbook.md` (new file) with target defaults for dev vs. production.

## Test Criteria
- [ ] **Benchmark Baseline:** Capture a pre-optimisation timing table (before/after) and attach to the issue for historical tracking.
- [ ] **Unit Tests:** Cover cache hits/misses, concurrency guards, and configuration validation.
- [ ] **Load Test:** Run a 10-query soak test via a script or notebook to verify throughput improvements and ensure no tool exceeds API limits.

## Notes
- Coordinate with Phase 5 runtime upgrades to keep optimisation hooks compatible with future LangChain changes.
- Ensure caching respects privacy expectations—avoid storing user queries beyond the configured TTL.

